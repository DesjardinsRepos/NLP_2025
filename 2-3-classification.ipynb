{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c508e677",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/desjardins/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/desjardins/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"TweetSentiment.csv\", encoding=\"ISO-8859-1\")[[\"text\", \"sentiment\"]]\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "\n",
    "contractions = {\n",
    "    \"don't\": \"do not\", \"doesn't\": \"does not\", \"can't\": \"cannot\", \"i'm\": \"i am\",\n",
    "    \"you're\": \"you are\", \"he's\": \"he is\", \"she's\": \"she is\", \"it's\": \"it is\",\n",
    "    \"we're\": \"we are\", \"they're\": \"they are\", \"isn't\": \"is not\", \"aren't\": \"are not\",\n",
    "    \"wasn't\": \"was not\", \"weren't\": \"were not\", \"won't\": \"will not\", \"wouldn't\": \"would not\",\n",
    "    \"couldn't\": \"could not\", \"shouldn't\": \"should not\", \"i've\": \"i have\", \"you've\": \"you have\",\n",
    "    \"we've\": \"we have\", \"they've\": \"they have\", \"i'll\": \"i will\", \"you'll\": \"you will\",\n",
    "    \"he'll\": \"he will\", \"she'll\": \"she will\", \"we'll\": \"we will\", \"they'll\": \"they will\",\n",
    "    \"there's\": \"there is\", \"that's\": \"that is\", \"what's\": \"what is\", \"who's\": \"who is\"\n",
    "}\n",
    "\n",
    "emoticon_dict = {\n",
    "    r\"(:-\\)|:\\)|=\\)|:\\]|=])\": \"SMILE\",\n",
    "    r\"(;-?\\)|;-?\\])\": \"WINK\",\n",
    "    r\"(:D|=D|;D)\": \"LAUGH\",\n",
    "    r\"(:\\(|:-\\(|=\\[|:\\[)\": \"SAD\",\n",
    "    r\"(:\\/|:-\\/)\": \"SKEPTICAL\",\n",
    "    r\"(<3)\": \"HEART\",\n",
    "    r\"(:3)\": \"CUTE\",\n",
    "    r\"(:P|:p|:-P|:-p|=P)\": \"PLAYFUL\",\n",
    "    r\"(:=)\": \"CONFUSED\",\n",
    "}\n",
    "\n",
    "def expand_contractions_fun(text):\n",
    "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(key) for key in contractions.keys()) + r')\\b')\n",
    "    return pattern.sub(lambda x: contractions[x.group()], text)\n",
    "\n",
    "def reduce_elongation_fun(word):\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1\\1', word)\n",
    "\n",
    "def preprocessing(\n",
    "    df,\n",
    "    text_col=\"text\",\n",
    "    lowercase=True,\n",
    "    expand_contractions=True,\n",
    "    remove_urls=True,\n",
    "    emoticon_normalization=True,\n",
    "    remove_mentions=True,\n",
    "    remove_punctuation=True,\n",
    "    preserve_ellipsis=True,\n",
    "    remove_numbers=False,\n",
    "    remove_non_ascii=True,\n",
    "    reduce_elongation=True,\n",
    "    remove_stopwords=True,\n",
    "    stemming=False,\n",
    "    lemmatization=False,\n",
    "    spelling_correction=False,\n",
    "):\n",
    "    stop_words = set(stopwords.words(\"english\")) if remove_stopwords else set()\n",
    "    stemmer = PorterStemmer() if stemming else None\n",
    "    lemmatizer = WordNetLemmatizer() if lemmatization else None\n",
    "\n",
    "    def clean_text(text):\n",
    "        if lowercase:\n",
    "            text = text.lower()\n",
    "        if expand_contractions:\n",
    "            text = expand_contractions_fun(text)\n",
    "        if remove_urls:\n",
    "            text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "\n",
    "        if emoticon_normalization:\n",
    "            for pattern, token in emoticon_dict.items():\n",
    "                text = re.sub(pattern, token, text, flags=re.IGNORECASE)\n",
    "\n",
    "        if remove_mentions: # maybe remove this\n",
    "            text = re.sub(r\"@\\w+\", \"\", text)\n",
    "\n",
    "        if remove_punctuation:\n",
    "            if preserve_ellipsis:\n",
    "                text = text.replace(\"...\", \"<ELLIPSIS>\")\n",
    "            text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "            if preserve_ellipsis:\n",
    "                text = text.replace(\"<ELLIPSIS>\", \"...\")\n",
    "\n",
    "        if remove_numbers:\n",
    "            text = re.sub(r\"\\d+\", \"\", text)\n",
    "        if remove_non_ascii:\n",
    "            text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
    "\n",
    "        tokens = text.split()\n",
    "\n",
    "        if reduce_elongation:\n",
    "            tokens = [reduce_elongation_fun(word) for word in tokens]\n",
    "        if remove_stopwords:\n",
    "            tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "        text = \" \".join(tokens)\n",
    "\n",
    "        if spelling_correction:\n",
    "            text = str(TextBlob(text).correct())\n",
    "            tokens = text.split()\n",
    "\n",
    "        if stemming:\n",
    "            tokens = [stemmer.stem(word) for word in tokens]\n",
    "        if lemmatization:\n",
    "            tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "        text = \" \".join(tokens)\n",
    "        return text\n",
    "\n",
    "    df[text_col] = df[text_col].astype(str).apply(clean_text)\n",
    "    return df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-2025-EqQOuGeB-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
