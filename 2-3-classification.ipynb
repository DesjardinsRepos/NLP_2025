{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c508e677",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/desjardins/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/desjardins/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "sentiment",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "processed",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "5a739de0-4bcf-4ef8-b766-e46dce2ff486",
       "rows": [
        [
         "0",
         " I`d have responded, if I were going",
         "neutral",
         "id responded going"
        ],
        [
         "1",
         " Sooo SAD I will miss you here in San Diego!!!",
         "negative",
         "soo sad miss san diego"
        ],
        [
         "2",
         "my boss is bullying me...",
         "negative",
         "boss bullying me.."
        ],
        [
         "3",
         " what interview! leave me alone",
         "negative",
         "interview leave alone"
        ],
        [
         "4",
         " Sons of ****, why couldn`t they put them on the releases we already bought",
         "negative",
         "sons CENSORED couldnt put releases already bought"
        ],
        [
         "5",
         "http://www.dothebouncy.com/smf - some shameless plugging for the best Rangers forum on earth",
         "neutral",
         "shameless plugging best rangers forum earth"
        ],
        [
         "6",
         "2am feedings for the baby are fun when he is all smiles and coos",
         "positive",
         "twoam feedings baby fun smiles coos"
        ],
        [
         "7",
         "Soooo high",
         "neutral",
         "soo high"
        ],
        [
         "8",
         " Both of you",
         "neutral",
         ""
        ],
        [
         "9",
         " Journey!? Wow... u just became cooler.  hehe... (is that possible!?)",
         "positive",
         "journey wow.. u became cooler hehe.. possible"
        ],
        [
         "10",
         " as much as i love to be hopeful, i reckon the chances are minimal =P i`m never gonna get my cake and stuff",
         "neutral",
         "much love hopeful reckon chances minimal PLAYFUL im never gonna get cake stuff"
        ],
        [
         "11",
         "I really really like the song Love Story by Taylor Swift",
         "positive",
         "really really like song love story taylor swift"
        ],
        [
         "12",
         "My Sharpie is running DANGERously low on ink",
         "negative",
         "sharpie running dangerously low ink"
        ],
        [
         "13",
         "i want to go to music tonight but i lost my voice.",
         "negative",
         "want go music tonight lost voice"
        ],
        [
         "14",
         "test test from the LG enV2",
         "neutral",
         "test test lg envtwo"
        ],
        [
         "15",
         "Uh oh, I am sunburned",
         "negative",
         "uh oh sunburned"
        ],
        [
         "16",
         " S`ok, trying to plot alternatives as we speak *sigh*",
         "negative",
         "sok trying plot alternatives speak sigh"
        ],
        [
         "17",
         "i`ve been sick for the past few days  and thus, my hair looks wierd.  if i didnt have a hat on it would look... http://tinyurl.com/mnf4kw",
         "negative",
         "ive sick past days thus hair looks wierd didnt hat would look.."
        ],
        [
         "18",
         "is back home now      gonna miss every one",
         "negative",
         "back home gonna miss every one"
        ],
        [
         "19",
         "Hes just not that into you",
         "neutral",
         "hes"
        ],
        [
         "20",
         " oh Marly, I`m so sorry!!  I hope you find her soon!! <3 <3",
         "neutral",
         "oh marly im sorry hope find soon HEART HEART"
        ],
        [
         "21",
         "Playing Ghost Online is really interesting. The new updates are Kirin pet and Metamorph for third job.  Can`t wait to have a dragon pet",
         "positive",
         "playing ghost online really interesting new updates kirin pet metamorph third job cant wait dragon pet"
        ],
        [
         "22",
         "is cleaning the house for her family who is comming later today..",
         "neutral",
         "cleaning house family comming later today"
        ],
        [
         "23",
         "gotta restart my computer .. I thought Win7 was supposed to put an end to the constant rebootiness",
         "neutral",
         "gotta restart computer thought winseven supposed put end constant rebootiness"
        ],
        [
         "24",
         "SEe waT I Mean bOuT FoLL0w fRiiDaYs... It`S cALLed LoSe f0LloWeRs FridAy... smH",
         "neutral",
         "see wat mean bout follzerow friidays.. called lose fzerollowers friday.. smh"
        ],
        [
         "25",
         "the free fillin` app on my ipod is fun, im addicted",
         "positive",
         "free fillin app ipod fun im addicted"
        ],
        [
         "26",
         "  I`m sorry.",
         "negative",
         "im sorry"
        ],
        [
         "27",
         "On the way to Malaysia...no internet access to Twit",
         "negative",
         "way malaysia..no internet access twit"
        ],
        [
         "28",
         "juss came backk from Berkeleyy ; omg its madd fun out there  havent been out there in a minute . whassqoodd ?",
         "positive",
         "juss came backk berkeleyy omg madd fun havent minute whassqoodd"
        ],
        [
         "29",
         "Went to sleep and there is a power cut in Noida  Power back up not working too",
         "negative",
         "went sleep power cut noida power back working"
        ],
        [
         "30",
         "I`m going home now. Have you seen my new twitter design? Quite....heavenly isn`****?",
         "positive",
         "im going home seen new twitter design quite..heavenly isnCENSORED"
        ],
        [
         "31",
         "i hope unni will make the audition . fighting dahye unni !",
         "positive",
         "hope unni make audition fighting dahye unni"
        ],
        [
         "32",
         " If it is any consolation I got my BMI tested hahaha it says I am obesed  well so much for being unhappy for about 10 minutes.",
         "negative",
         "consolation got bmi tested hahaha says obesed well much unhappy ten minutes"
        ],
        [
         "33",
         " That`s very funny.  Cute kids.",
         "positive",
         "thats funny cute kids"
        ],
        [
         "34",
         " Ahhh, I slept through the game.  I`m gonna try my best to watch tomorrow though. I hope we play Army.",
         "neutral",
         "ahh slept game im gonna try best watch tomorrow though hope play army"
        ],
        [
         "35",
         "Thats it, its the end. Tears for Fears vs Eric Prydz, DJ Hero   http://bit.ly/2Hpbg4",
         "neutral",
         "thats end tears fears vs eric prydz dj hero"
        ],
        [
         "36",
         "Born and raised in NYC and living in Texas for the past 10 years!  I still miss NY",
         "negative",
         "born raised nyc living texas past ten years still miss ny"
        ],
        [
         "37",
         "just in case you wonder, we are really busy today and this coming with with adding tons of new blogs and updates stay tuned",
         "neutral",
         "case wonder really busy today coming adding tons new blogs updates stay tuned"
        ],
        [
         "38",
         "i`m soooooo sleeeeepy!!! the last day o` school was today....sniffle....",
         "negative",
         "im soo sleepy last day school today..sniffle.."
        ],
        [
         "39",
         "A little happy for the wine jeje ok it`sm my free time so who cares, jaja i love this day",
         "positive",
         "little happy wine jeje ok itsm free time cares jaja love day"
        ],
        [
         "40",
         " Car not happy, big big dent in boot! Hoping theyre not going to write it off, crossing fingers and waiting",
         "neutral",
         "car happy big big dent boot hoping theyre going write crossing fingers waiting"
        ],
        [
         "41",
         "im an avid fan of **** magazine and i love your magazines",
         "positive",
         "im avid fan CENSORED magazine love magazines"
        ],
        [
         "42",
         "MAYDAY?!",
         "neutral",
         "mayday"
        ],
        [
         "43",
         "RATT ROCKED NASHVILLE TONITE..ONE THING SUCKED, NO ENCORE!  LIKE IN THE 80`S THEY STILL HAVE A FUN SHOW. PEARCY HAS THAT HOTT BAD BOY LOOK",
         "neutral",
         "ratt rocked nashville toniteone thing sucked encore like eightys still fun show pearcy hott bad boy look"
        ],
        [
         "44",
         " I love to! But I`m only available from 5pm.  and where dear? Would love to help  convert her vids.Ã¯Â¿Â½",
         "positive",
         "love im available fivepm dear would love help convert vids"
        ],
        [
         "45",
         "The girl in the hair salon asked me 'Shall I trim your eyebrows!' How old do I feel?",
         "neutral",
         "girl hair salon asked shall trim eyebrows old feel"
        ],
        [
         "46",
         "egh blah and boooooooooooo i dunno wanna go to work  HANGOVERS SUCKKKKKK Im a drunk mess!",
         "negative",
         "egh blah boo dunno wanna go work hangovers suckk im drunk mess"
        ],
        [
         "47",
         ":visiting my friendster and facebook",
         "neutral",
         "visiting friendster facebook"
        ],
        [
         "48",
         "i donbt like to peel prawns, i also dont like going shopping, running out of money and crawling round the car looking for more",
         "negative",
         "donbt like peel prawns also dont like going shopping running money crawling round car looking"
        ],
        [
         "49",
         " which case? I got a new one last week and I`m not thrilled at all with mine.",
         "negative",
         "case got new one last week im thrilled mine"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 27481
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>id responded going</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>negative</td>\n",
       "      <td>soo sad miss san diego</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>negative</td>\n",
       "      <td>boss bullying me..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>interview leave alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>sons CENSORED couldnt put releases already bought</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27476</th>\n",
       "      <td>wish we could come see u on Denver  husband l...</td>\n",
       "      <td>negative</td>\n",
       "      <td>wish could come see u denver husband lost job ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27477</th>\n",
       "      <td>I`ve wondered about rake to.  The client has ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>ive wondered rake client made clear net dont f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27478</th>\n",
       "      <td>Yay good for both of you. Enjoy the break - y...</td>\n",
       "      <td>positive</td>\n",
       "      <td>yay good enjoy break probably need hectic week...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27479</th>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>positive</td>\n",
       "      <td>worth CENSORED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27480</th>\n",
       "      <td>All this flirting going on - The ATG smiles...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>flirting going atg smiles yay hugs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27481 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text sentiment  \\\n",
       "0                    I`d have responded, if I were going   neutral   \n",
       "1          Sooo SAD I will miss you here in San Diego!!!  negative   \n",
       "2                              my boss is bullying me...  negative   \n",
       "3                         what interview! leave me alone  negative   \n",
       "4       Sons of ****, why couldn`t they put them on t...  negative   \n",
       "...                                                  ...       ...   \n",
       "27476   wish we could come see u on Denver  husband l...  negative   \n",
       "27477   I`ve wondered about rake to.  The client has ...  negative   \n",
       "27478   Yay good for both of you. Enjoy the break - y...  positive   \n",
       "27479                         But it was worth it  ****.  positive   \n",
       "27480     All this flirting going on - The ATG smiles...   neutral   \n",
       "\n",
       "                                               processed  \n",
       "0                                     id responded going  \n",
       "1                                 soo sad miss san diego  \n",
       "2                                     boss bullying me..  \n",
       "3                                  interview leave alone  \n",
       "4      sons CENSORED couldnt put releases already bought  \n",
       "...                                                  ...  \n",
       "27476  wish could come see u denver husband lost job ...  \n",
       "27477  ive wondered rake client made clear net dont f...  \n",
       "27478  yay good enjoy break probably need hectic week...  \n",
       "27479                                     worth CENSORED  \n",
       "27480                 flirting going atg smiles yay hugs  \n",
       "\n",
       "[27481 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"TweetSentiment.csv\", encoding=\"ISO-8859-1\")[[\"text\", \"sentiment\"]]\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from num2words import num2words\n",
    "\n",
    "contractions = {\n",
    "    \"don't\": \"do not\", \"doesn't\": \"does not\", \"can't\": \"cannot\", \"i'm\": \"i am\",\n",
    "    \"you're\": \"you are\", \"he's\": \"he is\", \"she's\": \"she is\", \"it's\": \"it is\",\n",
    "    \"we're\": \"we are\", \"they're\": \"they are\", \"isn't\": \"is not\", \"aren't\": \"are not\",\n",
    "    \"wasn't\": \"was not\", \"weren't\": \"were not\", \"won't\": \"will not\", \"wouldn't\": \"would not\",\n",
    "    \"couldn't\": \"could not\", \"shouldn't\": \"should not\", \"i've\": \"i have\", \"you've\": \"you have\",\n",
    "    \"we've\": \"we have\", \"they've\": \"they have\", \"i'll\": \"i will\", \"you'll\": \"you will\",\n",
    "    \"he'll\": \"he will\", \"she'll\": \"she will\", \"we'll\": \"we will\", \"they'll\": \"they will\",\n",
    "    \"there's\": \"there is\", \"that's\": \"that is\", \"what's\": \"what is\", \"who's\": \"who is\"\n",
    "}\n",
    "\n",
    "emoticon_dict = {\n",
    "    r\"(:-\\)|:\\)|=\\)|:\\]|=])\": \"SMILE\",\n",
    "    r\"(;-?\\)|;-?\\])\": \"WINK\",\n",
    "    r\"(:D|=D|;D)\": \"LAUGH\",\n",
    "    r\"(:\\(|:-\\(|=\\[|:\\[)\": \"SAD\",\n",
    "    r\"(:\\/|:-\\/)\": \"SKEPTICAL\",\n",
    "    r\"(<3)\": \"HEART\",\n",
    "    r\"(:3)\": \"CUTE\",\n",
    "    r\"(:P|:p|:-P|:-p|=P)\": \"PLAYFUL\",\n",
    "    r\"(:=)\": \"CONFUSED\",\n",
    "}\n",
    "\n",
    "def expand_contractions_fun(text):\n",
    "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(key) for key in contractions.keys()) + r')\\b')\n",
    "    return pattern.sub(lambda x: contractions[x.group()], text)\n",
    "\n",
    "def reduce_elongation_fun(word):\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1\\1', word)\n",
    "\n",
    "def preprocessing(\n",
    "    df,\n",
    "    text_col=\"text\",\n",
    "    lowercase=True,\n",
    "    expand_contractions=True,\n",
    "    remove_urls=True,\n",
    "    emoticon_normalization=True,\n",
    "    detect_censored=True,\n",
    "    remove_mentions=True,\n",
    "    remove_punctuation=True,\n",
    "    preserve_ellipsis=True,\n",
    "    remove_numbers=False,\n",
    "    convert_numbers=True,\n",
    "    remove_non_ascii=True,\n",
    "    reduce_elongation=True,\n",
    "    remove_stopwords=True,\n",
    "    stemming=False,\n",
    "    lemmatization=False,\n",
    "    spelling_correction=False,\n",
    "    strip_multispace=True,\n",
    "):\n",
    "    stop_words = set(stopwords.words(\"english\")) if remove_stopwords else set()\n",
    "    stemmer = PorterStemmer() if stemming else None\n",
    "    lemmatizer = WordNetLemmatizer() if lemmatization else None\n",
    "\n",
    "    def clean_text(text):\n",
    "        if lowercase:\n",
    "            text = text.lower()\n",
    "        if expand_contractions:\n",
    "            text = expand_contractions_fun(text)\n",
    "        if remove_urls:\n",
    "            text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "\n",
    "        if emoticon_normalization:\n",
    "            for pattern, token in emoticon_dict.items():\n",
    "                text = re.sub(pattern, token, text, flags=re.IGNORECASE)\n",
    "\n",
    "        if detect_censored:\n",
    "            text = re.sub(r\"\\*{2,}\", \"CENSORED\", text)\n",
    "        if remove_mentions: # maybe remove this\n",
    "            text = re.sub(r\"@\\w+\", \"\", text)\n",
    "\n",
    "        if preserve_ellipsis:\n",
    "            text = text.replace(\"...\", \"ELLIPSISTOKEN\")\n",
    "        if remove_punctuation:\n",
    "            text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "        if preserve_ellipsis:\n",
    "            text = text.replace(\"ELLIPSISTOKEN\", \"...\")\n",
    "\n",
    "        if convert_numbers:\n",
    "            text = re.sub(r\"\\d+\", lambda m: num2words(int(m.group())), text)\n",
    "        elif remove_numbers:\n",
    "            text = re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "        if remove_non_ascii:\n",
    "            text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
    "\n",
    "        tokens = text.split()\n",
    "\n",
    "        if reduce_elongation:\n",
    "            tokens = [reduce_elongation_fun(word) for word in tokens]\n",
    "        if remove_stopwords:\n",
    "            tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "        text = \" \".join(tokens)\n",
    "\n",
    "        if spelling_correction:\n",
    "            text = str(TextBlob(text).correct())\n",
    "            tokens = text.split()\n",
    "\n",
    "        if stemming:\n",
    "            tokens = [stemmer.stem(word) for word in tokens]\n",
    "        if lemmatization:\n",
    "            tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "        text = \" \".join(tokens)\n",
    "\n",
    "        if strip_multispace:\n",
    "            text = re.sub(r\"\\s{2,}\", \" \", text).strip()\n",
    "\n",
    "        return text\n",
    "\n",
    "    df[\"text\"] = df[text_col].astype(str).apply(clean_text)\n",
    "    return df\n",
    "\n",
    "\n",
    "processed_df = preprocessing(df)\n",
    "processed_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d5303b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Using Count Vectorizer ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.58      0.71      0.64      1556\n",
      "     neutral       0.59      0.49      0.53      2224\n",
      "    positive       0.66      0.68      0.67      1717\n",
      "\n",
      "    accuracy                           0.61      5497\n",
      "   macro avg       0.61      0.63      0.61      5497\n",
      "weighted avg       0.61      0.61      0.61      5497\n",
      "\n",
      "\n",
      "=== Using TF-IDF Vectorizer ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.59      0.71      0.64      1556\n",
      "     neutral       0.60      0.49      0.54      2224\n",
      "    positive       0.66      0.69      0.67      1717\n",
      "\n",
      "    accuracy                           0.61      5497\n",
      "   macro avg       0.62      0.63      0.62      5497\n",
      "weighted avg       0.61      0.61      0.61      5497\n",
      "\n",
      "\n",
      "=== Using Binary Vectorizer ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      0.71      0.63      1556\n",
      "     neutral       0.59      0.49      0.53      2224\n",
      "    positive       0.67      0.68      0.68      1717\n",
      "\n",
      "    accuracy                           0.61      5497\n",
      "   macro avg       0.61      0.62      0.61      5497\n",
      "weighted avg       0.61      0.61      0.61      5497\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# vectorization + train naive bayes\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import resample\n",
    "from collections import Counter\n",
    "\n",
    "df = pd.read_csv(\"TweetSentiment.csv\", encoding=\"ISO-8859-1\")[[\"text\", \"sentiment\"]]\n",
    "df = preprocessing(df)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"text\"], df[\"sentiment\"], test_size=0.2, stratify=df[\"sentiment\"], random_state=42\n",
    ")\n",
    "\n",
    "def balance_classes(X, y, method='oversample'):\n",
    "    X = pd.Series(X).tolist()\n",
    "    y = pd.Series(y).tolist()\n",
    "    \n",
    "    df_combined = pd.DataFrame({'text': X, 'label': y})\n",
    "    class_counts = df_combined['label'].value_counts()\n",
    "    max_size = class_counts.max()\n",
    "    min_size = class_counts.min()\n",
    "\n",
    "    dfs = []\n",
    "    for label in class_counts.index:\n",
    "        df_label = df_combined[df_combined['label'] == label]\n",
    "        if method == 'oversample':\n",
    "            df_label = resample(df_label, replace=True, n_samples=max_size, random_state=42)\n",
    "        elif method == 'undersample':\n",
    "            df_label = resample(df_label, replace=False, n_samples=min_size, random_state=42)\n",
    "        dfs.append(df_label)\n",
    "\n",
    "    df_balanced = pd.concat(dfs).sample(frac=1, random_state=42)\n",
    "    return df_balanced['text'], df_balanced['label']\n",
    "\n",
    "\n",
    "X_train_bal, y_train_bal = balance_classes(X_train, y_train, method='oversample')\n",
    "\n",
    "vectorizers = {\n",
    "    \"Count Vectorizer\": CountVectorizer(),\n",
    "    \"TF-IDF Vectorizer\": TfidfVectorizer(),\n",
    "    \"Binary Vectorizer\": CountVectorizer(binary=True)\n",
    "}\n",
    "\n",
    "for name, vectorizer in vectorizers.items():\n",
    "    print(f\"\\n=== Using {name} ===\")\n",
    "    X_train_vec = vectorizer.fit_transform(X_train_bal)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X_train_vec, y_train_bal)\n",
    "\n",
    "    y_pred = model.predict(X_test_vec)\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-2025-EqQOuGeB-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
