{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c508e677",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/desjardins/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/desjardins/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sentiment",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "7e2c2071-c6b0-46ba-acea-e8aa3fde1ea7",
       "rows": [
        [
         "0",
         "id responded going",
         "neutral"
        ],
        [
         "1",
         "soo sad miss san diego",
         "negative"
        ],
        [
         "2",
         "boss bullying me..",
         "negative"
        ],
        [
         "3",
         "interview leave alone",
         "negative"
        ],
        [
         "4",
         "sons CENSORED couldnt put releases already bought",
         "negative"
        ],
        [
         "5",
         "shameless plugging best rangers forum earth",
         "neutral"
        ],
        [
         "6",
         "twoam feedings baby fun smiles coos",
         "positive"
        ],
        [
         "7",
         "soo high",
         "neutral"
        ],
        [
         "8",
         "",
         "neutral"
        ],
        [
         "9",
         "journey wow.. u became cooler hehe.. possible",
         "positive"
        ],
        [
         "10",
         "much love hopeful reckon chances minimal PLAYFUL im never gonna get cake stuff",
         "neutral"
        ],
        [
         "11",
         "really really like song love story taylor swift",
         "positive"
        ],
        [
         "12",
         "sharpie running dangerously low ink",
         "negative"
        ],
        [
         "13",
         "want go music tonight lost voice",
         "negative"
        ],
        [
         "14",
         "test test lg envtwo",
         "neutral"
        ],
        [
         "15",
         "uh oh sunburned",
         "negative"
        ],
        [
         "16",
         "sok trying plot alternatives speak sigh",
         "negative"
        ],
        [
         "17",
         "ive sick past days thus hair looks wierd didnt hat would look..",
         "negative"
        ],
        [
         "18",
         "back home gonna miss every one",
         "negative"
        ],
        [
         "19",
         "hes",
         "neutral"
        ],
        [
         "20",
         "oh marly im sorry hope find soon HEART HEART",
         "neutral"
        ],
        [
         "21",
         "playing ghost online really interesting new updates kirin pet metamorph third job cant wait dragon pet",
         "positive"
        ],
        [
         "22",
         "cleaning house family comming later today",
         "neutral"
        ],
        [
         "23",
         "gotta restart computer thought winseven supposed put end constant rebootiness",
         "neutral"
        ],
        [
         "24",
         "see wat mean bout follzerow friidays.. called lose fzerollowers friday.. smh",
         "neutral"
        ],
        [
         "25",
         "free fillin app ipod fun im addicted",
         "positive"
        ],
        [
         "26",
         "im sorry",
         "negative"
        ],
        [
         "27",
         "way malaysia..no internet access twit",
         "negative"
        ],
        [
         "28",
         "juss came backk berkeleyy omg madd fun havent minute whassqoodd",
         "positive"
        ],
        [
         "29",
         "went sleep power cut noida power back working",
         "negative"
        ],
        [
         "30",
         "im going home seen new twitter design quite..heavenly isnCENSORED",
         "positive"
        ],
        [
         "31",
         "hope unni make audition fighting dahye unni",
         "positive"
        ],
        [
         "32",
         "consolation got bmi tested hahaha says obesed well much unhappy ten minutes",
         "negative"
        ],
        [
         "33",
         "thats funny cute kids",
         "positive"
        ],
        [
         "34",
         "ahh slept game im gonna try best watch tomorrow though hope play army",
         "neutral"
        ],
        [
         "35",
         "thats end tears fears vs eric prydz dj hero",
         "neutral"
        ],
        [
         "36",
         "born raised nyc living texas past ten years still miss ny",
         "negative"
        ],
        [
         "37",
         "case wonder really busy today coming adding tons new blogs updates stay tuned",
         "neutral"
        ],
        [
         "38",
         "im soo sleepy last day school today..sniffle..",
         "negative"
        ],
        [
         "39",
         "little happy wine jeje ok itsm free time cares jaja love day",
         "positive"
        ],
        [
         "40",
         "car happy big big dent boot hoping theyre going write crossing fingers waiting",
         "neutral"
        ],
        [
         "41",
         "im avid fan CENSORED magazine love magazines",
         "positive"
        ],
        [
         "42",
         "mayday",
         "neutral"
        ],
        [
         "43",
         "ratt rocked nashville toniteone thing sucked encore like eightys still fun show pearcy hott bad boy look",
         "neutral"
        ],
        [
         "44",
         "love im available fivepm dear would love help convert vids",
         "positive"
        ],
        [
         "45",
         "girl hair salon asked shall trim eyebrows old feel",
         "neutral"
        ],
        [
         "46",
         "egh blah boo dunno wanna go work hangovers suckk im drunk mess",
         "negative"
        ],
        [
         "47",
         "visiting friendster facebook",
         "neutral"
        ],
        [
         "48",
         "donbt like peel prawns also dont like going shopping running money crawling round car looking",
         "negative"
        ],
        [
         "49",
         "case got new one last week im thrilled mine",
         "negative"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 27481
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id responded going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>soo sad miss san diego</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>boss bullying me..</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>interview leave alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sons CENSORED couldnt put releases already bought</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27476</th>\n",
       "      <td>wish could come see u denver husband lost job ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27477</th>\n",
       "      <td>ive wondered rake client made clear net dont f...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27478</th>\n",
       "      <td>yay good enjoy break probably need hectic week...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27479</th>\n",
       "      <td>worth CENSORED</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27480</th>\n",
       "      <td>flirting going atg smiles yay hugs</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27481 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text sentiment\n",
       "0                                     id responded going   neutral\n",
       "1                                 soo sad miss san diego  negative\n",
       "2                                     boss bullying me..  negative\n",
       "3                                  interview leave alone  negative\n",
       "4      sons CENSORED couldnt put releases already bought  negative\n",
       "...                                                  ...       ...\n",
       "27476  wish could come see u denver husband lost job ...  negative\n",
       "27477  ive wondered rake client made clear net dont f...  negative\n",
       "27478  yay good enjoy break probably need hectic week...  positive\n",
       "27479                                     worth CENSORED  positive\n",
       "27480                 flirting going atg smiles yay hugs   neutral\n",
       "\n",
       "[27481 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing Function\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"TweetSentiment.csv\", encoding=\"ISO-8859-1\")[[\"text\", \"sentiment\"]]\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from num2words import num2words\n",
    "\n",
    "contractions = {\n",
    "    \"don't\": \"do not\", \"doesn't\": \"does not\", \"can't\": \"cannot\", \"i'm\": \"i am\",\n",
    "    \"you're\": \"you are\", \"he's\": \"he is\", \"she's\": \"she is\", \"it's\": \"it is\",\n",
    "    \"we're\": \"we are\", \"they're\": \"they are\", \"isn't\": \"is not\", \"aren't\": \"are not\",\n",
    "    \"wasn't\": \"was not\", \"weren't\": \"were not\", \"won't\": \"will not\", \"wouldn't\": \"would not\",\n",
    "    \"couldn't\": \"could not\", \"shouldn't\": \"should not\", \"i've\": \"i have\", \"you've\": \"you have\",\n",
    "    \"we've\": \"we have\", \"they've\": \"they have\", \"i'll\": \"i will\", \"you'll\": \"you will\",\n",
    "    \"he'll\": \"he will\", \"she'll\": \"she will\", \"we'll\": \"we will\", \"they'll\": \"they will\",\n",
    "    \"there's\": \"there is\", \"that's\": \"that is\", \"what's\": \"what is\", \"who's\": \"who is\"\n",
    "}\n",
    "\n",
    "emoticon_dict = {\n",
    "    r\"(:-\\)|:\\)|=\\)|:\\]|=])\": \"SMILE\",\n",
    "    r\"(;-?\\)|;-?\\])\": \"WINK\",\n",
    "    r\"(:D|=D|;D)\": \"LAUGH\",\n",
    "    r\"(:\\(|:-\\(|=\\[|:\\[)\": \"SAD\",\n",
    "    r\"(:\\/|:-\\/)\": \"SKEPTICAL\",\n",
    "    r\"(<3)\": \"HEART\",\n",
    "    r\"(:3)\": \"CUTE\",\n",
    "    r\"(:P|:p|:-P|:-p|=P)\": \"PLAYFUL\",\n",
    "    r\"(:=)\": \"CONFUSED\",\n",
    "}\n",
    "\n",
    "def expand_contractions_fun(text):\n",
    "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(key) for key in contractions.keys()) + r')\\b')\n",
    "    return pattern.sub(lambda x: contractions[x.group()], text)\n",
    "\n",
    "def reduce_elongation_fun(word):\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1\\1', word)\n",
    "\n",
    "def preprocessing(\n",
    "    df,\n",
    "    text_col=\"text\",\n",
    "    lowercase=True,\n",
    "    expand_contractions=True,\n",
    "    remove_urls=True,\n",
    "    emoticon_normalization=True,\n",
    "    detect_censored=True,\n",
    "    remove_mentions=True,\n",
    "    remove_punctuation=True,\n",
    "    preserve_ellipsis=True,\n",
    "    remove_numbers=False,\n",
    "    convert_numbers=True,\n",
    "    remove_non_ascii=True,\n",
    "    reduce_elongation=True,\n",
    "    remove_stopwords=True,\n",
    "    stemming=False,\n",
    "    lemmatization=False,\n",
    "    spelling_correction=False,\n",
    "    strip_multispace=True,\n",
    "):\n",
    "    stop_words = set(stopwords.words(\"english\")) if remove_stopwords else set()\n",
    "    stemmer = PorterStemmer() if stemming else None\n",
    "    lemmatizer = WordNetLemmatizer() if lemmatization else None\n",
    "\n",
    "    def clean_text(text):\n",
    "        if lowercase:\n",
    "            text = text.lower()\n",
    "        if expand_contractions:\n",
    "            text = expand_contractions_fun(text)\n",
    "        if remove_urls:\n",
    "            text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "\n",
    "        if emoticon_normalization:\n",
    "            for pattern, token in emoticon_dict.items():\n",
    "                text = re.sub(pattern, token, text, flags=re.IGNORECASE)\n",
    "\n",
    "        if detect_censored:\n",
    "            text = re.sub(r\"\\*{2,}\", \"CENSORED\", text)\n",
    "        if remove_mentions: # maybe remove this\n",
    "            text = re.sub(r\"@\\w+\", \"\", text)\n",
    "\n",
    "        if preserve_ellipsis:\n",
    "            text = text.replace(\"...\", \"ELLIPSISTOKEN\")\n",
    "        if remove_punctuation:\n",
    "            text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "        if preserve_ellipsis:\n",
    "            text = text.replace(\"ELLIPSISTOKEN\", \"...\")\n",
    "\n",
    "        if convert_numbers:\n",
    "            text = re.sub(r\"\\d+\", lambda m: num2words(int(m.group())), text)\n",
    "        elif remove_numbers:\n",
    "            text = re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "        if remove_non_ascii:\n",
    "            text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
    "\n",
    "        tokens = text.split()\n",
    "\n",
    "        if reduce_elongation:\n",
    "            tokens = [reduce_elongation_fun(word) for word in tokens]\n",
    "        if remove_stopwords:\n",
    "            tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "        text = \" \".join(tokens)\n",
    "\n",
    "        if spelling_correction:\n",
    "            text = str(TextBlob(text).correct())\n",
    "            tokens = text.split()\n",
    "\n",
    "        if stemming:\n",
    "            tokens = [stemmer.stem(word) for word in tokens]\n",
    "        if lemmatization:\n",
    "            tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "        text = \" \".join(tokens)\n",
    "\n",
    "        if strip_multispace:\n",
    "            text = re.sub(r\"\\s{2,}\", \" \", text).strip()\n",
    "\n",
    "        return text\n",
    "\n",
    "    df[\"text\"] = df[text_col].astype(str).apply(clean_text)\n",
    "    return df\n",
    "\n",
    "\n",
    "processed_df = preprocessing(df)\n",
    "processed_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d5303b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer (BoW):\n",
      "  Accuracy: 0.7753\n",
      "  Recall:   0.7753\n",
      "  F1 Score: 0.7722\n",
      "\n",
      "TF-IDF:\n",
      "  Accuracy: 0.7771\n",
      "  Recall:   0.7771\n",
      "  F1 Score: 0.7743\n",
      "\n",
      "Binary Vectorizer:\n",
      "  Accuracy: 0.7783\n",
      "  Recall:   0.7783\n",
      "  F1 Score: 0.7752\n",
      "\n",
      "BoW with N-grams:\n",
      "  Accuracy: 0.8357\n",
      "  Recall:   0.8357\n",
      "  F1 Score: 0.8335\n",
      "\n",
      "HashingVectorizer:\n",
      "  Accuracy: 0.6939\n",
      "  Recall:   0.6939\n",
      "  F1 Score: 0.6934\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes: Different Vectorizers\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "df = pd.read_csv(\"TweetSentiment.csv\", encoding=\"ISO-8859-1\")[[\"text\", \"sentiment\"]]\n",
    "# df = preprocessing(df)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "max_size = df[\"sentiment\"].value_counts().max()\n",
    "df_balanced = pd.concat([\n",
    "    resample(class_df, replace=True, n_samples=max_size, random_state=42)\n",
    "    for _, class_df in df.groupby(\"sentiment\")\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_balanced[\"text\"], df_balanced[\"sentiment\"],\n",
    "    test_size=0.2, stratify=df_balanced[\"sentiment\"], random_state=42\n",
    ")\n",
    "\n",
    "vectorizers = {\n",
    "    \"CountVectorizer (BoW)\": CountVectorizer(),\n",
    "    \"TF-IDF\": TfidfVectorizer(),\n",
    "    \"Binary Vectorizer\": CountVectorizer(binary=True),\n",
    "    \"BoW with N-grams\": CountVectorizer(ngram_range=(1, 2)),\n",
    "    \"HashingVectorizer\": HashingVectorizer(n_features=5000, alternate_sign=False)\n",
    "}\n",
    "\n",
    "def evaluate(X_train_vec, X_test_vec, y_train, y_test):\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X_train_vec, y_train)\n",
    "    y_pred = model.predict(X_test_vec)\n",
    "    return accuracy_score(y_test, y_pred), recall_score(y_test, y_pred, average=\"macro\"), f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "for name, vectorizer in vectorizers.items():\n",
    "    X_train_vec = vectorizer.fit_transform(X_train) if hasattr(vectorizer, \"fit_transform\") else vectorizer.transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    acc, rec, f1 = evaluate(X_train_vec, X_test_vec, y_train, y_test)\n",
    "    print(f\"{name}:\\n  Accuracy: {acc:.4f}\\n  Recall:   {rec:.4f}\\n  F1 Score: {f1:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8db31ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.7643    0.8475    0.8038      2223\n",
      "     neutral     0.7512    0.6353    0.6884      2224\n",
      "    positive     0.8065    0.8431    0.8244      2224\n",
      "\n",
      "    accuracy                         0.7753      6671\n",
      "   macro avg     0.7740    0.7753    0.7722      6671\n",
      "weighted avg     0.7740    0.7753    0.7722      6671\n",
      "\n",
      "CountVectorizer (BoW):\n",
      "  Accuracy: 0.7753\n",
      "  Recall:   0.7753\n",
      "  F1 Score: 0.7722\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.7701    0.8426    0.8047      2223\n",
      "     neutral     0.7490    0.6439    0.6925      2224\n",
      "    positive     0.8075    0.8449    0.8258      2224\n",
      "\n",
      "    accuracy                         0.7771      6671\n",
      "   macro avg     0.7755    0.7771    0.7743      6671\n",
      "weighted avg     0.7755    0.7771    0.7743      6671\n",
      "\n",
      "TF-IDF:\n",
      "  Accuracy: 0.7771\n",
      "  Recall:   0.7771\n",
      "  F1 Score: 0.7743\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.7659    0.8507    0.8061      2223\n",
      "     neutral     0.7611    0.6403    0.6955      2224\n",
      "    positive     0.8052    0.8440    0.8241      2224\n",
      "\n",
      "    accuracy                         0.7783      6671\n",
      "   macro avg     0.7774    0.7783    0.7752      6671\n",
      "weighted avg     0.7774    0.7783    0.7752      6671\n",
      "\n",
      "Binary Vectorizer:\n",
      "  Accuracy: 0.7783\n",
      "  Recall:   0.7783\n",
      "  F1 Score: 0.7752\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8161    0.9042    0.8579      2223\n",
      "     neutral     0.8325    0.7127    0.7679      2224\n",
      "    positive     0.8594    0.8903    0.8746      2224\n",
      "\n",
      "    accuracy                         0.8357      6671\n",
      "   macro avg     0.8360    0.8357    0.8335      6671\n",
      "weighted avg     0.8360    0.8357    0.8334      6671\n",
      "\n",
      "BoW with N-grams:\n",
      "  Accuracy: 0.8357\n",
      "  Recall:   0.8357\n",
      "  F1 Score: 0.8335\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.6996    0.7395    0.7190      2223\n",
      "     neutral     0.6226    0.6007    0.6114      2224\n",
      "    positive     0.7582    0.7415    0.7497      2224\n",
      "\n",
      "    accuracy                         0.6939      6671\n",
      "   macro avg     0.6934    0.6939    0.6934      6671\n",
      "weighted avg     0.6934    0.6939    0.6934      6671\n",
      "\n",
      "HashingVectorizer:\n",
      "  Accuracy: 0.6939\n",
      "  Recall:   0.6939\n",
      "  F1 Score: 0.6934\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes: Different Vectorizers\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "df = pd.read_csv(\"TweetSentiment.csv\", encoding=\"ISO-8859-1\")[[\"text\", \"sentiment\"]]\n",
    "# df = preprocessing(df)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "max_size = df[\"sentiment\"].value_counts().max()\n",
    "df_balanced = pd.concat([\n",
    "    resample(class_df, replace=True, n_samples=max_size, random_state=42)\n",
    "    for _, class_df in df.groupby(\"sentiment\")\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_balanced[\"text\"], df_balanced[\"sentiment\"],\n",
    "    test_size=0.2, stratify=df_balanced[\"sentiment\"], random_state=42\n",
    ")\n",
    "\n",
    "vectorizers = {\n",
    "    \"CountVectorizer (BoW)\": CountVectorizer(),\n",
    "    \"TF-IDF\": TfidfVectorizer(),\n",
    "    \"Binary Vectorizer\": CountVectorizer(binary=True),\n",
    "    \"BoW with N-grams\": CountVectorizer(ngram_range=(1, 2)),\n",
    "    \"HashingVectorizer\": HashingVectorizer(n_features=5000, alternate_sign=False)\n",
    "}\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate(X_train_vec, X_test_vec, y_train, y_test):\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X_train_vec, y_train)\n",
    "    y_pred = model.predict(X_test_vec)\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "    return (\n",
    "        accuracy_score(y_test, y_pred),\n",
    "        recall_score(y_test, y_pred, average=\"macro\"),\n",
    "        f1_score(y_test, y_pred, average=\"macro\")\n",
    "    )\n",
    "for name, vectorizer in vectorizers.items():\n",
    "    X_train_vec = vectorizer.fit_transform(X_train) if hasattr(vectorizer, \"fit_transform\") else vectorizer.transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    acc, rec, f1 = evaluate(X_train_vec, X_test_vec, y_train, y_test)\n",
    "    print(f\"{name}:\\n  Accuracy: {acc:.4f}\\n  Recall:   {rec:.4f}\\n  F1 Score: {f1:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3957433",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/desjardins/.cache/pypoetry/virtualenvs/nlp-2025-EqQOuGeB-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorizer: Accuracy 0.8384 | Recall 0.8384 | F1 Score 0.8388\n",
      "TF-IDF Vectorizer: Accuracy 0.7961 | Recall 0.7961 | F1 Score 0.7966\n",
      "Binary Count Vectorizer: Accuracy 0.8404 | Recall 0.8404 | F1 Score 0.8407\n",
      "Bag of 2-grams: Accuracy 0.8390 | Recall 0.8390 | F1 Score 0.8402\n",
      "Hashing Vectorizer: Accuracy 0.7215 | Recall 0.7215 | F1 Score 0.7222\n",
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
      "Word2Vec Embeddings: Accuracy 0.5600 | Recall 0.5600 | F1 Score 0.5615\n",
      "BERT Sentence Embeddings: Accuracy 0.6994 | Recall 0.6995 | F1 Score 0.6992\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes: Different Vectorizers - Part 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import gensim.downloader as api\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "df = pd.read_csv(\"TweetSentiment.csv\", encoding=\"ISO-8859-1\")[[\"text\", \"sentiment\"]]\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "max_size = df[\"sentiment\"].value_counts().max()\n",
    "df_balanced = pd.concat([\n",
    "    resample(class_df, replace=True, n_samples=max_size, random_state=42)\n",
    "    for _, class_df in df.groupby(\"sentiment\")\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_balanced[\"text\"], df_balanced[\"sentiment\"],\n",
    "    test_size=0.2, stratify=df_balanced[\"sentiment\"], random_state=42\n",
    ")\n",
    "\n",
    "vectorizers = {\n",
    "    \"Count Vectorizer\": CountVectorizer(),\n",
    "    \"TF-IDF Vectorizer\": TfidfVectorizer(),\n",
    "    \"Binary Count Vectorizer\": CountVectorizer(binary=True),\n",
    "    \"Bag of 2-grams\": CountVectorizer(ngram_range=(2, 2)),\n",
    "    \"Hashing Vectorizer\": HashingVectorizer(n_features=5000, alternate_sign=False),\n",
    "}\n",
    "\n",
    "def train_eval_vectorizer(name, vectorizer):\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    clf.fit(X_train_vec, y_train)\n",
    "    y_pred = clf.predict(X_test_vec)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    print(f\"{name}: Accuracy {acc:.4f} | Recall {rec:.4f} | F1 Score {f1:.4f}\")\n",
    "\n",
    "for name, vec in vectorizers.items():\n",
    "    train_eval_vectorizer(name, vec)\n",
    "\n",
    "w2v = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "def get_w2v_embedding(text):\n",
    "    tokens = text.split()\n",
    "    vecs = [w2v[word] for word in tokens if word in w2v]\n",
    "    return np.mean(vecs, axis=0) if vecs else np.zeros(100)\n",
    "\n",
    "X_train_w2v = np.vstack(X_train.apply(get_w2v_embedding))\n",
    "X_test_w2v = np.vstack(X_test.apply(get_w2v_embedding))\n",
    "\n",
    "clf_w2v = LogisticRegression(max_iter=1000, random_state=42)\n",
    "clf_w2v.fit(X_train_w2v, y_train)\n",
    "y_pred_w2v = clf_w2v.predict(X_test_w2v)\n",
    "print(f\"Word2Vec Embeddings: Accuracy {accuracy_score(y_test, y_pred_w2v):.4f} | Recall {recall_score(y_test, y_pred_w2v, average='macro'):.4f} | F1 Score {f1_score(y_test, y_pred_w2v, average='macro'):.4f}\")\n",
    "\n",
    "model_bert = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "X_train_bert = model_bert.encode(X_train.tolist(), convert_to_numpy=True)\n",
    "X_test_bert = model_bert.encode(X_test.tolist(), convert_to_numpy=True)\n",
    "\n",
    "clf_bert = LogisticRegression(max_iter=1000, random_state=42)\n",
    "clf_bert.fit(X_train_bert, y_train)\n",
    "y_pred_bert = clf_bert.predict(X_test_bert)\n",
    "print(f\"BERT Sentence Embeddings: Accuracy {accuracy_score(y_test, y_pred_bert):.4f} | Recall {recall_score(y_test, y_pred_bert, average='macro'):.4f} | F1 Score {f1_score(y_test, y_pred_bert, average='macro'):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8b636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes: Different Preprocessing\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec24155a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed-Forward: Different Vectorizers\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c83d95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base accuracy (no preprocessing): 0.6843\n",
      "lowercase                : +0.0005\n",
      "expand_contractions      : -0.0011\n",
      "remove_urls              : +0.0011\n",
      "emoticon_normalization   : -0.0013\n",
      "detect_censored          : -0.0002\n",
      "remove_mentions          : -0.0007\n",
      "remove_punctuation       : -0.0040\n",
      "preserve_ellipsis        : -0.0009\n",
      "remove_numbers           : -0.0002\n",
      "convert_numbers          : -0.0004\n",
      "remove_non_ascii         : -0.0011\n",
      "reduce_elongation        : +0.0027\n",
      "remove_stopwords         : -0.0069\n",
      "stemming                 : -0.0066\n",
      "lemmatization            : +0.0031\n",
      "spelling_correction      : -0.0133\n",
      "strip_multispace         : -0.0015\n"
     ]
    }
   ],
   "source": [
    "# Feed-Forward: Different Preprocessing\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "df = pd.read_csv(\"TweetSentiment.csv\", encoding=\"ISO-8859-1\")[[\"text\", \"sentiment\"]]\n",
    "df.dropna(subset=[\"text\", \"sentiment\"], inplace=True)\n",
    "df[\"label\"] = df[\"sentiment\"].astype(\"category\").cat.codes\n",
    "\n",
    "def prepare_data(df):\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    X = vectorizer.fit_transform(df[\"text\"]).toarray()\n",
    "    y = df[\"label\"].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=SEED\n",
    "    )\n",
    "    return (\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(X_test, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.long),\n",
    "        torch.tensor(y_test, dtype=torch.long),\n",
    "        X_train.shape[1]\n",
    "    )\n",
    "\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 3)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "def train_and_eval(X_train, y_train, X_test, y_test, input_dim):\n",
    "    model = FeedForwardNN(input_dim)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    model.train()\n",
    "    for epoch in range(3):\n",
    "        for i in range(0, len(X_train), 64):\n",
    "            batch_x = X_train[i:i+64]\n",
    "            batch_y = y_train[i:i+64]\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = torch.argmax(model(X_test), dim=1)\n",
    "    return accuracy_score(y_test, preds)\n",
    "\n",
    "options = [\n",
    "    \"lowercase\", \"expand_contractions\", \"remove_urls\", \"emoticon_normalization\",\n",
    "    \"detect_censored\", \"remove_mentions\", \"remove_punctuation\", \"preserve_ellipsis\",\n",
    "    \"remove_numbers\", \"convert_numbers\", \"remove_non_ascii\", \"reduce_elongation\",\n",
    "    \"remove_stopwords\", \"stemming\", \"lemmatization\", \"spelling_correction\", \"strip_multispace\"\n",
    "]\n",
    "\n",
    "df_raw = df.copy()\n",
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw, input_dim_raw = prepare_data(df_raw)\n",
    "acc_base = train_and_eval(X_train_raw, y_train_raw, X_test_raw, y_test_raw, input_dim_raw)\n",
    "print(f\"Base accuracy (no preprocessing): {acc_base:.4f}\")\n",
    "\n",
    "for opt in options:\n",
    "    kwargs = {k: False for k in options}\n",
    "    kwargs[opt] = True\n",
    "    df_pre = preprocessing(df.copy(), **kwargs)\n",
    "    X_train_pre, X_test_pre, y_train_pre, y_test_pre, input_dim_pre = prepare_data(df_pre)\n",
    "    acc = train_and_eval(X_train_pre, y_train_pre, X_test_pre, y_test_pre, input_dim_pre)\n",
    "    delta = acc - acc_base\n",
    "    print(f\"{opt:25s}: {'+' if delta >= 0 else ''}{delta:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-2025-EqQOuGeB-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
